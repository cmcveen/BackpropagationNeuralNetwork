# BackpropagationNeuralNetwork

The task for this assignment was to implement a neural network that utilized a backpropagation algorithm to learn and classify data that represented hand written Hindu-Arabic numerals

The network that was implemented used three layers. 64 nodes in the input layer, 100 in the hidden layer, and 10 in the output layer. 64 nodes were decided upon for the input layer because there were 64 inputs, so 1 node per input. For the output layer, 10 nodes were decided upon as the output could be one of 10 options. So if the first node in the layer fired, the result would be a 0, the second would mean a 1, and so forth to the last node firing meaning a 9.  The amount of nodes in the hidden layer were experimented on throughout the process of testing the network. The initial value started at 15 for no reason other than to include a number of hidden nodes. From there trials were done with smaller, larger, and much larger nodes. Some of the values used included the numbers 32, 128, and 300 before finally settling on 100 nodes. The criteria for choosing this value was mostly based off of what produced more accurate results without taking too long to compute.  Initially 128 worked just as well for 100, without much difference between the two, 100 was just chosen because it is a smaller number of nodes in the layer.

Finally for the momentum, an initial pass was done without using momentum at all, so a value of 0. A second attempt with the data was done utilizing a final momentum value of 0.2. Values were experimented with to see the effect of the momentum on the network, initially starting at 0.1, and then varying with other numbers such as 0.4, 0.5, and 0.7. The higher the momentum value was, the higher amount of errors that seemed to be produced. When 0.7, 0.5 and 0.4 were used, higher rates of errors were seen with an increased global error as well. However with 0.2, the error rate on classification seemed consistent to the trials down without momentum. What was noted though with these trials was that on occasion the global error rate on the training phase was higher than without the momentum, but still managed approximately an equal amount of classification errors, sometimes even better than without momentum. 

Finally the results of this network seemed to indicate a good amount of proper classification of the inputs. Without momentum, the network made approximately 65 to 110 errors, or approximately 93-96% accuracy. With a momentum of 0.2 similar results were seen as well.
